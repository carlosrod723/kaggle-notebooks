{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-16T17:46:30.809901Z","iopub.execute_input":"2024-05-16T17:46:30.810951Z","iopub.status.idle":"2024-05-16T17:46:31.739212Z","shell.execute_reply.started":"2024-05-16T17:46:30.810908Z","shell.execute_reply":"2024-05-16T17:46:31.738117Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**The Lunar Lander Environment (Reinforcement Learning)**\n\nThe lunar lander environment is a reinforcement learning problem where the goal is to train an agent (a lander) to land on a designated landing pad on the surface of the moon. Reinforcement learning is a type of machine learning technique where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions. The goal is to maximize the cumulative reward over time. The environment is provided by OpenAI Gym, which is a toolkit for developing and comparing reinforcement learning algorithms.\n\n**Problem Statement**\n\nThe goal of the Lunar Lander environment is to implement a Deep Q-Learning alorithm using TensorFlow to land the lunar lander safely on the landing pad on the surface of the moon. The landing pad is designated by two flag poles and its center is at coordinates (0,0), but the lander is also allowed to land outside of the landing pad. The lander starts at the top center of the environment with a random initial force applied to its center of mass and has infinite fuel. \n\n**Markov Decision Process (MDP)**\n\nThe lunar lander environment is modeled as a Markov Decision Process, which is a formal framework for describing reinforcement learning problems. In an MDP, the future depends only on the current state and not on the history of how the agent got there. The agent interacts with the environment by choosing actions, observing new states, and receiving rewards. The lunar lander environment consists of the following:\n\n* **State**- The current position, velocity, and other relevant variables that describe the lander's situation.\n\n* **Action**- The agent's choice of applying force to the lander's engines in a particular direction.\n\n* **Reward**- The feedback signal that the agent receives based on its actions (e.g., a positive reward for a successful landing, a negative reward for crashing or running out of fuel).\n\n* **Transition Function**- The dynamics that determine how the lander's state changes based on the current state and the agent's action.\n\n**Deep Q-Learning**\n\nDeep Q-Learning is a popular reinforcement learning algorithm that combines Q-learning (a value-based method) with deep neural networks. The agent learns to estimate the expected future rewards (Q-values) for each state-action pair, and then takes the action with the highest Q-value in a given state.\n\n**TensorFlow**\n\nTensorFlow is a popular open-source machine learning library developed by Google. In the lunar lander environment, TensorFlow can be used to build and train the deep neural networks that approximate the Q-function for the agent.\n\n**Agent Actions**\n\nIt's important to note that the agent has 4 discrete actions available: \n\nThe agent has four discrete actions available, each corresponding to a numerical value:\n\n1. Do nothing= 0\n2. Fire right engine= 1\n3. Fire main engine= 2\n4. Fire left engine= 3","metadata":{}},{"cell_type":"code","source":"# Import all necessary packages and libraries\nimport numpy as np\nimport time\nimport random\nfrom collections import deque, namedtuple\n\nimport gym\nfrom gym.wrappers import NormalizeObservation\nfrom gym.wrappers import TimeLimit\nfrom gym.wrappers import ClipAction\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.losses import MSE\nfrom tensorflow.keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:46:31.740993Z","iopub.execute_input":"2024-05-16T17:46:31.741628Z","iopub.status.idle":"2024-05-16T17:46:43.380298Z","shell.execute_reply.started":"2024-05-16T17:46:31.741604Z","shell.execute_reply":"2024-05-16T17:46:43.379222Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-16 17:46:33.954112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-16 17:46:33.954229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-16 17:46:34.098865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setting the random seed for TensorFlow\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:46:43.381697Z","iopub.execute_input":"2024-05-16T17:46:43.382304Z","iopub.status.idle":"2024-05-16T17:46:43.385926Z","shell.execute_reply.started":"2024-05-16T17:46:43.382279Z","shell.execute_reply":"2024-05-16T17:46:43.385214Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter selection\nMEMORY_SIZE = 100_000\nGAMMA = 0.995\nALPHA = 1e-3\nNUM_STEPS_FOR_UPDATE = 4","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:46:43.387694Z","iopub.execute_input":"2024-05-16T17:46:43.388123Z","iopub.status.idle":"2024-05-16T17:46:43.416543Z","shell.execute_reply.started":"2024-05-16T17:46:43.388101Z","shell.execute_reply":"2024-05-16T17:46:43.415651Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Observation Space & Action Space**\n\nIn reinforcement learning, the observation space defines what information the agent receives from the environment at each step. It can include things like the position, velocity, and orientation of the Lunar Lander. \n\nThe action space defines the set of actions the agent can take. For the Lunar Lander, there are 4 discrete actions, which were already mentioned previously. \n\nThis information is crucial for designing the neural network. The size of the state vector (8 in this case) determines the number of input neurons to the network and the number of possible actions (4), determines the number of output neurons, where each neuron represents the probability of taking a particular action.","metadata":{}},{"cell_type":"code","source":"# Create the Lunar Lander environment\n!apt-get update\n!apt-get install -y swig\n!pip install gym[box2d]\nenv = gym.make('LunarLander-v2')\n\n# Get state and action sizes\nstate_size = env.observation_space.shape[0]  # Number of state values\nnum_actions = env.action_space.n  # Number of possible actions\n\nprint('State size:', state_size)\nprint('Number of actions:', num_actions)\n\nenv.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:46:43.417766Z","iopub.execute_input":"2024-05-16T17:46:43.418240Z","iopub.status.idle":"2024-05-16T17:47:38.792564Z","shell.execute_reply.started":"2024-05-16T17:46:43.418216Z","shell.execute_reply":"2024-05-16T17:47:38.791386Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Get:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1225 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]       \nHit:3 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:6 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [21.0 kB]\nGet:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [2964 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1438 kB]\nHit:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease   \nGet:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3608 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1502 kB]\nGet:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3669 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3758 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1205 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4143 kB]\nFetched 22.5 MB in 2s (11.9 MB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  swig4.0\nSuggested packages:\n  swig-doc swig-examples swig4.0-examples swig4.0-doc\nThe following NEW packages will be installed:\n  swig swig4.0\n0 upgraded, 2 newly installed, 0 to remove and 63 not upgraded.\nNeed to get 1086 kB of archives.\nAfter this operation, 5413 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1081 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5528 B]\nFetched 1086 kB in 1s (1257 kB/s)\nSelecting previously unselected package swig4.0.\n(Reading database ... 110195 files and directories currently installed.)\nPreparing to unpack .../swig4.0_4.0.1-5build1_amd64.deb ...\nUnpacking swig4.0 (4.0.1-5build1) ...\nSelecting previously unselected package swig.\nPreparing to unpack .../swig_4.0.1-5build1_all.deb ...\nUnpacking swig (4.0.1-5build1) ...\nSetting up swig4.0 (4.0.1-5build1) ...\nSetting up swig (4.0.1-5build1) ...\nProcessing triggers for man-db (2.9.1-1) ...\nRequirement already satisfied: gym[box2d] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\nCollecting box2d-py==2.3.5 (from gym[box2d])\n  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pygame==2.1.0 (from gym[box2d])\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\nCollecting swig==4.* (from gym[box2d])\n  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\nDownloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: box2d-py\n  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=494460 sha256=95c2b8e557a2588950a2249c0225a8b789ad867d5b351b4bddf291cfce900e60\n  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\nSuccessfully built box2d-py\nInstalling collected packages: swig, box2d-py, pygame\nSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.2.1\nState size: 8\nNumber of actions: 4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**State Size (8)**\n\nThe Lunar Lander's state is a vector that describes its current situation. It consists of 8 components:\n\n1. **x position**- The horizontal position of the lander.\n\n2. **y position**- The vertical position of the lander.\n\n3. **x velocity**- The horizontal velocity of the lander.\n\n4. **y velocity**- The vertical velocity of the lander.\n\n5. **angle**- The angle of the lander in radians.\n\n6. **angular velocity**- The angular velocity of the lander in radians per second.\n\n7. **left leg contact**- A boolean (1 or 0) indicating if the left leg is in contact with the ground.\n\n8. **right leg contact**- A boolean (1 or 0) indicating if the right leg is in contact with the ground.\n\n\n**Resetting the Environment**\n\nI'm going to reset the environment to its initial state using *env.reset()*. This ensures that each episode begins from a defined point, preventing the agent from continuing from a previous state. In addition to resetting the environment, *env.reset()* returns the initial observation of the agent in this new episode. This observation is crucial as it serves as the starting point for the agent's decision-making process.\n\nIt's important to note that the initial observation is crucial as it serves as the starting point for the agent's decision-making process. Furthermore, if the environment isn't reset between episodes, the agent might start from an arbitrary state reached in the previous episode. This can introduce bias into the learning process because the agent's actions and the resulting rewards would depend on the final state of the previous episode. Resetting helps ensure that the learning process starts fresh fresh (yes, double fresh) with each episode. ","metadata":{}},{"cell_type":"code","source":"# Reset the environment\ninitial_state = env.reset()","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.794791Z","iopub.execute_input":"2024-05-16T17:47:38.795080Z","iopub.status.idle":"2024-05-16T17:47:38.800989Z","shell.execute_reply.started":"2024-05-16T17:47:38.795055Z","shell.execute_reply":"2024-05-16T17:47:38.799295Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Action**\n\nOnce the environment is reset, the agent can start taking action. It's important to note that the agent can only take one action per time step. ","metadata":{}},{"cell_type":"code","source":"# Select an action\naction= 2  # Fire main engine\n\n# Run a single time step of the environment\nnext_state, reward, terminated, truncated, info = env.step(action)\n\n# Print results\nprint('Next state: ', next_state)\nprint('Reward: ', reward)\nprint('Terminated: ', terminated)  # True if episode is over due to termination\nprint('Truncated: ', truncated)  # True if the episode is over due to truncation\nprint('Info: ', info)  # Diagnostic information\n\n# Update the current state\ncurrent_state = next_state","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.801806Z","iopub.execute_input":"2024-05-16T17:47:38.802127Z","iopub.status.idle":"2024-05-16T17:47:38.817169Z","shell.execute_reply.started":"2024-05-16T17:47:38.802106Z","shell.execute_reply":"2024-05-16T17:47:38.815626Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Next state:  [-0.01184988  1.3984901  -0.59748363 -0.27277595  0.01385155  0.13912511\n  0.          0.        ]\nReward:  0.3355417830521333\nTerminated:  False\nTruncated:  False\nInfo:  {}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Analyzing the Output**\n\nThe 'Next State' values are just the corresponding values from the 8 dimensional array representing the new state of the lander after taking action (in this case, firing the main engine). \n\nThe 'Reward' received is a negative value, indicating that the action was not favorable according to the environment's reward function. This could be because the lander consumed fuel, moved further away from the landing pad, or experienced some other undesirable outcome.\n\nThe 'Terminated' flag indicates where the episode has ended. In this case, the result is 'False' meaning that the lander has neither landed successfully nor crashed yet, and the episode is still ongoing.\n\nThe 'Truncated' flag indicates whether the episode was terminated prematurely (due to reaching a time limit, for example). In this case, it's 'False', meaning the episode is progressing naturally. \n\n**Core Concepts**\n\nThe following information includes core concepts pertaining to the lunar lander environment. These concepts are crucial for the Deep Q-Learning algorithm because they address challenges in reinforcement learning, such as instability, inefficient learning, and the need for accurate Q-value approximation.\n\n1. **Q-Networks**- In reinforcement learning, the Q-function, denoted as Q(s, a), estimates the expected future reward for taking action a in state s. In essence, it says how \"good\" it is to take a specific action in a given situation. In DQN, a neural network is used to approximate this Q-function. This network takes the state as input and outputs Q-values for each possible action. In the lunar lander environment, the Q-Network takes the current state (which could be the position, velocity, angle, etc.) as input and outputs the estimated Q-values for each possible action (applying force in different directions, for example). The goal is to learn the optimal Q-values, which will guide the agent to take actions that maximize the cumulative reward (in other words, achieve a successful landing).\n\n2. **The Bellman Equation**- The Bellman equation is a fundamental equation in reinforcement learning that defines the relationship between the Q-value of a state-action pair and the Q-value of the next state-action pair. It expresses the idea that the optimal Q-value for a state-action pair is equal to the immediate reward received for taking that action plus the discounted maximum Q-value of the next state over all possible actions. In the lunar lander environment, the Bellman equation allows us to update the Q-values based on the observed rewards and transitions, enabling the Q-Network to learn the optimal policy iteratively.This equation helps the agent understand the value of taking certain actions in specific situations.\n\n3. **Target Networks**- A second neural network, identical in structure to the Q-network, is introduced. This is called the target network. Its purpose is to provide stable target values for training the Q-network. The target network's weights are updated periodically by copying the weights of the Q-network. In this environment, using a Target Network helps stabilize the training process and improves the convergence of the Q-Network towards the optimal policy.\n\n4. **Experience Replay**- This is a technique where the agent stores its experiences (state, action, reward, next state) in a memory buffer. During training, batches of experiences are randomly sampled from this buffer to update the Q-network. This helps break correlations between consecutive experiences and improves learning stability. Experience Replay improves the efficiency of learning by reusing past experiences and decorrelating the samples used for training the Q-Network.\n\nDeep Q-Learning faces instability issues due to the constantly changing nature of the Q-values during learning. By using a target network and experience replay, it stabilizes the learning process. The target network provides consistent target values, and experience replay helps decorrelate updates and reduce variance.\n\nIn simpler terms, through iterative updates using the Bellman equation and the target network, the Q-network gradually learns to approximate the optimal Q-function, leading to better decision-making by the agent.\n\nBasically, the training process goes as follows:\n\n1. The agent starts by exploring randomly, collecting experiences.\n\n2. Experiences are stored in the replay buffer.\n\n3. Batches of experiences are sampled from the buffer.\n\n4. For each experience, the target network is used to calculate the target Q-value.\n\n5. The Q-network is updated to minimize the difference between its predicted Q-value and the target Q-value.\n\n6. Periodically, the target network's weights are updated to match the Q-network's weights.\n\nWith these concepts in mind, let's proceed to implement the DQN algorithm. ","metadata":{}},{"cell_type":"code","source":"# Create the Q-Network\nq_network = Sequential([\n    Input(shape=(state_size,)),\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=num_actions, activation='linear'),\n])\n\n# Create the target Q-Network\ntarget_q_network = Sequential([\n    Input(shape=(state_size,)),\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=num_actions, activation='linear'),\n])\n\n# Set up Adam optimizer with learning rate\noptimizer = Adam(learning_rate=ALPHA)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.818342Z","iopub.execute_input":"2024-05-16T17:47:38.818709Z","iopub.status.idle":"2024-05-16T17:47:38.916200Z","shell.execute_reply.started":"2024-05-16T17:47:38.818676Z","shell.execute_reply":"2024-05-16T17:47:38.915248Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"What I've done here is define both the main Q-Network and the Target Network. In addition to this, the Adam optimizer is now ready to update the Q-Network's weights during training.\n\n**Unit Test**\n\nNow I'm going to create a unit test to verify the code. This is an important step because it allows me to catch errors in the development process and it lets me ensure that my code is behaving as expected. So before moving on with the rest of the DQN algorithm, the unit test will verify if the neural networks and the optimizer have been correctly implemented. ","metadata":{}},{"cell_type":"code","source":"# Unit test\nassert q_network.count_params()== target_q_network.count_params() # Networks should have the same number of parameters\nassert isinstance(optimizer, Adam) # Optimizer should be Adam\nassert optimizer.learning_rate== ALPHA # Learning rate should be ALPHA\n\n# Print model summaries\nprint(q_network.summary())\nprint(target_q_network.summary())","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.917376Z","iopub.execute_input":"2024-05-16T17:47:38.917706Z","iopub.status.idle":"2024-05-16T17:47:38.952341Z","shell.execute_reply.started":"2024-05-16T17:47:38.917678Z","shell.execute_reply":"2024-05-16T17:47:38.951569Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,996\u001b[0m (19.52 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,996</span> (19.52 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Model Summaries** \n\nThe model summaries confirm both the Q-network and target Q-network have identical architectures: sequential models with an input layer of 8 neurons (matching the state size), two hidden layers of 64 neurons each using ReLU activation, and an output layer of 4 neurons (matching the number of actions) using linear activation. Both models have the same number of trainable parameters, indicating correct implementation for the DQN algorithm!","metadata":{}},{"cell_type":"code","source":"# Defining experience\nclass Experience:\n    def __init__(self, state, action, reward, next_state, terminated, truncated):\n        # Check if the state is a scalar or a nested sequence\n        if isinstance(state, (list, tuple, np.ndarray)):\n            self.state = np.array(state).flatten()  # Convert state to a flattened NumPy array\n        else:\n            self.state = np.array([state])  # Convert scalar state to a NumPy array\n\n        self.action = action\n        self.reward = reward\n\n        # Check if the next_state is a scalar or a nested sequence\n        if isinstance(next_state, (list, tuple, np.ndarray)):\n            self.next_state = np.array(next_state).flatten()  # Convert next_state to a flattened NumPy array\n        else:\n            self.next_state = np.array([next_state])  # Convert scalar next_state to a NumPy array\n\n        self.terminated = terminated\n        self.truncated = truncated","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.954691Z","iopub.execute_input":"2024-05-16T17:47:38.954965Z","iopub.status.idle":"2024-05-16T17:47:38.960859Z","shell.execute_reply.started":"2024-05-16T17:47:38.954943Z","shell.execute_reply":"2024-05-16T17:47:38.960066Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Deep Q-Network Loss**\n\nNow, I'm going to calculate the loss for the algorithm. This loss measures how well the Q-network predicts the expected future reward (Q-values) for given states and actions.\n\nBasically, the following function takes a batch of experiences as input, unpacks the relevant information (states, actions, rewards, etc.), and calculates the target Q-values using the Bellman equation and the target network. The target Q-values are then compared with the Q-values predicted by the main Q-network, and the mean squared error (MSE) between them is computed. This MSE loss is the output of the function and is used to update the Q-network's weights during training, ultimately improving the agent's decision-making capabilities.","metadata":{}},{"cell_type":"code","source":"# Define the loss function\ndef compute_loss(experiences, gamma, q_network, target_q_network):\n    states_batch, action_batch, reward_batch, next_states_batch, terminated_batch, truncated_batch = experiences\n    \n    # Convert states and next_states to TensorFlow tensors\n    processed_states = tf.convert_to_tensor(states_batch, dtype=tf.float32)\n    next_states_batch = tf.convert_to_tensor(next_states_batch, dtype=tf.float32)\n    \n    # Convert other experience components to NumPy arrays\n    action_batch = np.array(action_batch)\n    reward_batch = np.array(reward_batch)\n    terminated_batch = np.array(terminated_batch)\n    truncated_batch = np.array(truncated_batch)\n    \n    # Compute Q values for current states and actions using the Q-network\n    q_values = q_network(processed_states)\n    q_values = tf.reduce_sum(q_values * tf.one_hot(action_batch, q_values.shape[1]), axis=1)\n    \n    # Compute Q values for next states using the target Q-network\n    next_q_values = target_q_network(next_states_batch)\n    max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n    \n    # Compute target Q values\n    target_q_values = reward_batch + gamma * max_next_q_values * (1 - terminated_batch) * (1 - truncated_batch)\n    \n    # Compute loss\n    loss = tf.reduce_mean(tf.square(q_values - target_q_values))\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.961654Z","iopub.execute_input":"2024-05-16T17:47:38.961893Z","iopub.status.idle":"2024-05-16T17:47:38.975537Z","shell.execute_reply.started":"2024-05-16T17:47:38.961873Z","shell.execute_reply":"2024-05-16T17:47:38.974505Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Update the weights of the Q-Network\ndef agent_learn(experiences, gamma, q_network, target_q_network):\n    states_batch, action_batch, reward_batch, next_states_batch, terminated_batch, truncated_batch = zip(*experiences)\n    \n    # Convert states and next_states to TensorFlow tensors\n    processed_states = tf.convert_to_tensor(states_batch, dtype=tf.float32)\n    next_states_batch = tf.convert_to_tensor(next_states_batch, dtype=tf.float32)\n\n    # Convert other experience components to NumPy arrays\n    action_batch = np.array(action_batch)\n    reward_batch = np.array(reward_batch)\n    terminated_batch = np.array(terminated_batch)\n    truncated_batch = np.array(truncated_batch)\n\n    # Compute the loss and perform gradient descent\n    with tf.GradientTape() as tape:\n        loss = compute_loss((processed_states, action_batch, reward_batch, next_states_batch, terminated_batch, truncated_batch), gamma, q_network, target_q_network)\n    gradients = tape.gradient(loss, q_network.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n\n    # Soft update the weights of the Target Q-Network\n    tau = 0.001\n    for t, e in zip(target_q_network.trainable_variables, q_network.trainable_variables):\n        t.assign(tau * e + (1 - tau) * t)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.976925Z","iopub.execute_input":"2024-05-16T17:47:38.977240Z","iopub.status.idle":"2024-05-16T17:47:38.990094Z","shell.execute_reply.started":"2024-05-16T17:47:38.977213Z","shell.execute_reply":"2024-05-16T17:47:38.989289Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Loop-a-Loop**\n\nNow, let's create a loop to allow the agent to take many consecutive actions during an episode.\n\nReinforcement learning tasks are generally framed as episodes, where the agent starts in an initial state, interacts with the environment by taking actions, receives rewards, and eventually reaches a terminal state (in this case, successfully landing the lunar lander).\n\nThe agent needs to learn a policy that maps states to actions in a way that maximizes cumulative rewards over time. This learning happens through repeated interactions with the environment, where the agent explores different actions and learns from their consequences.\n\nTo learn effectively, reinforcement learning algorithms typically need a significant amount of experience data, consisting of transitions between states, actions, rewards, and next states. By running multiple steps within an episode, the agent can collect this valuable experience data.\n\nEssentially, the core idea here is to create a training loop that allows the DQN agent to interact with the Lunar Lander environment, collect experiences, learn from those experiences, and gradually improve its policy (the strategy for choosing actions) over time. \n\nIn the training loop, I will introduce the Epsilon-Greedy Concept. This concept is a simple yet effective approach to balance exploration and exploitation in reinforcement learning. For the lunar lander environment, it means that with a probability of epsilon, the agent will take a random action (exploration) to discover potentially better strategies, and with a probability of (1-epsilon), the agent will choose the action with the highest predicted Q-value according to its current knowledge (exploitation). This balance ensures that the agent both learns about its environment and makes use of the knowledge it has gained.","metadata":{}},{"cell_type":"code","source":"# Training loop\nstart_time = time.time()\nreplay_buffer = deque(maxlen=MEMORY_SIZE)\n\nnum_episodes = 2000\nmax_timesteps = 1000\ntotal_point_history = []\nnum_p_av = 100\nepsilon = 1.0\nepsilon_decay = 0.995  # Add a decay rate for epsilon\nBATCH_SIZE = 32\n\n# Set the target network weights equal to the Q-Network weights\ntarget_q_network.set_weights(q_network.get_weights())\n\nfor episode in range(num_episodes):\n    total_points = 0\n    state = env.reset()\n\n    for t in range(max_timesteps):\n        # Choose action using ε-greedy exploration\n        if np.random.rand() <= epsilon:\n            action = env.action_space.sample()  # Explore- random action\n        else:\n            state_qn = np.expand_dims(state, axis=0)\n            q_values = q_network.predict(state_qn)\n            action = np.argmax(q_values[0])  # Exploit- choose action with max Q-value\n\n        # Take action and observe the environment\n        next_state, reward, terminated, truncated, info = env.step(action)\n\n        # Store experience in replay buffer\n        experience = Experience(state, action, reward, next_state, terminated, truncated)\n        replay_buffer.append(experience)\n\n        # Update the Q-Network\n        if len(replay_buffer) > BATCH_SIZE:  # Wait until we have enough experiences\n            if t % NUM_STEPS_FOR_UPDATE == 0:\n                # Sample random mini-batch of experience tuples from replay buffer\n                experiences = random.sample(replay_buffer, BATCH_SIZE)\n                agent_learn(experiences, GAMMA, q_network, target_q_network)\n\n        # Update the Target Network every 100 steps\n        if t % 100 == 0:\n            target_q_network.set_weights(q_network.get_weights())\n\n        state = next_state\n        total_points += reward\n\n        if terminated or truncated:\n            break  # End episode\n\n    total_point_history.append(total_points)\n\n    # Print average reward every 100 episodes\n    if episode % 100 == 0:\n        avg_points = np.mean(total_point_history[-num_p_av:])\n        print(f'\\rEpisode {episode + 1} | Average Points: {avg_points:.2f}')\n\n    # Decay epsilon for better exploitation\n    epsilon *= epsilon_decay\n\n    if avg_points > 200.0:\n        print(f'\\n\\nEnvironment solved in {episode + 1} episodes!')\n        break\n\ntotal_time = time.time() - start_time\nprint(f'\\nTotal Runtime: {total_time:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:47:38.990948Z","iopub.execute_input":"2024-05-16T17:47:38.991237Z","iopub.status.idle":"2024-05-16T17:47:39.732495Z","shell.execute_reply.started":"2024-05-16T17:47:38.991207Z","shell.execute_reply":"2024-05-16T17:47:39.731232Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Store experience in replay buffer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m experience \u001b[38;5;241m=\u001b[39m \u001b[43mExperience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mappend(experience)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Update the Q-Network\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mExperience.__init__\u001b[0;34m(self, state, action, reward, next_state, terminated, truncated)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, terminated, truncated):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Check if the state is a scalar or a nested sequence\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Convert state to a flattened NumPy array\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([state])  \u001b[38;5;66;03m# Convert scalar state to a NumPy array\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."],"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.","output_type":"error"}]},{"cell_type":"markdown","source":"After the training loop, the only step missing is to plot the results to visualize how the agent improved over training. Unfortunately, I have not been able to fix the error from the training loop. It's given me quite a headache. ","metadata":{}}]}